# -*- coding: utf-8 -*-
"""Employee Salary Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wlFlGOAJuv8apdQ-gyuww_qz4WB-PBCa
"""

# Basic libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For model building
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()

import io

# Load dataset
df = pd.read_csv("adult 3.csv")

# Display first 5 rows
df.head()

import pandas as pd
import io

# Load CSV
df = pd.read_csv(io.BytesIO(uploaded["adult 3 (1).csv"]), header=None)

# Add column names
df.columns = [
    'age', 'workclass', 'fnlwgt', 'education', 'educational-num', 'marital-status',
    'occupation', 'relationship', 'race', 'gender', 'capital-gain', 'capital-loss',
    'hours-per-week', 'native-country', 'income'
]

df.head()

# Replace '?' with NaN
df.replace(' ?', pd.NA, inplace=True)

# Check missing values
df.isna().sum()

# Drop rows with missing values
df.dropna(inplace=True)

# Reset index after drop
df.reset_index(drop=True, inplace=True)

print("Data shape after dropping missing values:", df.shape)

from sklearn.preprocessing import LabelEncoder

# Columns to encode
categorical_cols = df.select_dtypes(include='object').columns

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

print("Categorical columns encoded:", list(categorical_cols))

import matplotlib.pyplot as plt
import seaborn as sns
import math

numeric_cols = df.select_dtypes(include='number').columns

n_cols = 4
n_rows = math.ceil(len(numeric_cols) / n_cols)

plt.figure(figsize=(n_cols * 5, n_rows * 4))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(x=df[col])
    plt.title(col)

plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Age distribution
sns.histplot(df['age'], bins=30, kde=True)
plt.title("Age Distribution")
plt.show()

# Income count
sns.countplot(x='income', data=df)
plt.title("Income Class Distribution")
plt.show()

# Education level
sns.countplot(y='education', data=df, order=df['education'].value_counts().index)
plt.title("Education Level Frequency")
plt.show()

# Age vs Income
sns.boxplot(x='income', y='age', data=df)
plt.title("Age vs Income")
plt.show()

# Education vs Income
sns.boxplot(x='income', y='educational-num', data=df)
plt.title("Education Level (numeric) vs Income")
plt.show()

# Hours-per-week vs Income
sns.boxplot(x='income', y='hours-per-week', data=df)
plt.title("Working Hours vs Income")
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# Features and target
X = df.drop('income', axis=1)
y = df['income']

# Split into train and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training shape:", X_train.shape)
print("Testing shape:", X_test.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Models
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

models = {
    'Linear Regression': LinearRegression(),
    'Lasso Regression': Lasso(),
    'Ridge Regression': Ridge(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

from sklearn.metrics import mean_squared_error, r2_score

# Store results
results = []

for name, model in models.items():
    # Choose scaled or unscaled data depending on model
    if name in ['Linear Regression', 'Lasso Regression', 'Ridge Regression']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    results.append({
        'Model': name,
        'MSE': round(mse, 4),
        'RÂ²': round(r2, 4)
    })

# Show results
results_df = pd.DataFrame(results)
results_df.sort_values(by='RÂ²', ascending=False)

import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score

# Create a results dictionary
results = {
    "Model": ["Linear Regression", "Lasso Regression", "Ridge Regression", "Random Forest", "Gradient Boosting"],
    "MSE": [
        mean_squared_error(y_test, models['Linear Regression'].predict(X_test_scaled)),
        mean_squared_error(y_test, models['Lasso Regression'].predict(X_test_scaled)),
        mean_squared_error(y_test, models['Ridge Regression'].predict(X_test_scaled)),
        mean_squared_error(y_test, models['Random Forest'].predict(X_test)),
        mean_squared_error(y_test, models['Gradient Boosting'].predict(X_test))
    ],
    "RÂ²": [
        r2_score(y_test, models['Linear Regression'].predict(X_test_scaled)),
        r2_score(y_test, models['Lasso Regression'].predict(X_test_scaled)),
        r2_score(y_test, models['Ridge Regression'].predict(X_test_scaled)),
        r2_score(y_test, models['Random Forest'].predict(X_test)),
        r2_score(y_test, models['Gradient Boosting'].predict(X_test))
    ]
}

# Create DataFrame and sort by best RÂ²
results_df = pd.DataFrame(results).sort_values(by='RÂ²', ascending=False).reset_index(drop=True)

# Show results
results_df

import joblib

# Save model and scaler
joblib.dump(models['Gradient Boosting'], 'salary_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

from google.colab import files
files.download('salary_model.pkl')
files.download('scaler.pkl')

# Get user input
age = int(input("Enter Age (18â€“65): "))
education_num = int(input("Enter Education Level (1â€“16): "))
hours_per_week = int(input("Enter Hours per Week (1â€“100): "))
capital_gain = int(input("Enter Capital Gain (0â€“99999): "))
capital_loss = int(input("Enter Capital Loss (0â€“4356): "))

# Create input DataFrame
custom_input = pd.DataFrame([[age, 0, 0, 0, education_num, 0, 0, 0, 0, 0, capital_gain, capital_loss, hours_per_week, 0]],
                            columns=['age', 'workclass', 'fnlwgt', 'education', 'educational-num',
                                     'marital-status', 'occupation', 'relationship', 'race', 'gender',
                                     'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'])

# Select the best performing model
best_model = models['Gradient Boosting']

# Scale input
custom_scaled = scaler.transform(custom_input)

# Predict
prediction = best_model.predict(custom_scaled)

# Output result
print("\nðŸ”® Predicted Income Level:")
if prediction[0] == 1:
    print("ðŸ’° More than 50K")
else:
    print("ðŸ§¾ Less than or equal to 50K")

import joblib

# Save your trained model and scaler
joblib.dump(best_model, 'salary_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

"""## âœ… Project Conclusion

In this project, we built a Salary Prediction system using the Adult Income dataset. After preprocessing the data, handling missing values, encoding categorical variables, and performing feature scaling, we trained several regression models. The Gradient Boosting Regressor performed the best with an RÂ² score of approximately 0.45.

The model and scaler were saved using `joblib`, making it ready for deployment in applications like a Streamlit web app.

This project demonstrates a complete ML workflow: Data Preprocessing â†’ Model Training â†’ Evaluation â†’ Saving â†’ (optional) Deployment.

"""